import gradio as gr
import os
import json
from pathlib import Path
import tempfile
import shutil
from datetime import datetime
import re

# Import the core music generation functionality
from music_generator_core import (
    graph, GraphState, EmotionResult, MusicBrief,
    dump, generate_with_replicate_strict,
    analyze_emotion_node, compose_brief_node  # Import new nodes
)

from music_generate_image import (
    graph as image_graph, 
    GraphState as ImageGraphState,
    analyze_emotion_from_image_node,
    compose_brief_node as image_compose_brief_node
)

def _safe_filename(s: str, max_len: int = 80) -> str:
    # í•œê¸€/ì˜ë¬¸/ìˆ«ì/ê³µë°±/ì¼ë¶€ ê¸°í˜¸ë§Œ í—ˆìš© â†’ ë‚˜ë¨¸ì§€ëŠ” _
    s = s.strip()
    s = re.sub(r"[^0-9A-Za-zê°€-í£ _\-\(\)\[\]\.]", "_", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s[:max_len]

def _label_with_ext(prefix: str, path: str) -> str:
    return f"{prefix} â€” {os.path.basename(path)}"


def _title_from_brief(brief: MusicBrief) -> str:
    # ì˜ˆ: "Joyful - C major - 130bpm - regulate:uplift"
    mode = next((t for t in (brief.style_tags or []) if t.startswith("regulate:")), None)
    parts = [brief.mood, brief.key, f"{brief.bpm}bpm"]
    if mode: parts.append(mode)
    return " - ".join([p for p in parts if p])

def _rename_generated_file(src_path: str, brief: MusicBrief) -> tuple[str, str]:
    """
    ìƒì„±ëœ íŒŒì¼ì„ 'ì œëª©_YYYYMMDD_HHMMSS.ext'ë¡œ rename.
    - íŒŒì¼ëª…: OS í˜¸í™˜ ìœ„í•´ ì•ˆì „í•œ ë¬¸ìë§Œ ì‚¬ìš© (':' ê°™ì€ ê±´ '_'ë¡œ)
    - ë¼ë²¨(í‘œì‹œìš© ì œëª©): ì½œë¡  ê·¸ëŒ€ë¡œ ìœ ì§€ + ë’¤ì— '_YYYYMMDD_HHMMSS'
    ë°˜í™˜: (ìƒˆ ê²½ë¡œ, í‘œì‹œìš© ì œëª©)
    """
    base_title = _title_from_brief(brief) or "Therapeutic Music"
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")

    # íŒŒì¼ëª…ìš©(ì•ˆì „ ë¬¸ìë§Œ)
    safe_base = _safe_filename(base_title)   # ':' â†’ '_'ë¡œ ì¹˜í™˜ë¨
    root, ext = os.path.splitext(src_path)
    dst_name = f"{safe_base}_{ts}{ext or '.wav'}"
    dst = os.path.join(os.path.dirname(src_path), dst_name)
    shutil.move(src_path, dst)

    # í‘œì‹œìš© ì œëª©(ì½œë¡  ìœ ì§€) â†’ "â€¦regulate:uplift_20250918_174045"
    display_title = f"{base_title}_{ts}"
    return dst, display_title

# === ìƒë‹¨ì— ìœ í‹¸ ì¶”ê°€ ===
def _norm(s: str) -> str:
    return " ".join((s or "").split()).strip()


def get_usage_md():
    return """### ğŸ“˜ ì‚¬ìš©ë²•
- **ğŸ” ê°ì • ë¶„ì„**: ê°ì • ìƒíƒœë§Œ ë¶„ì„
- **ğŸ“‹ ê°ì • ë¶„ì„ + ìŒì•… ì„¤ê³„**: ê°ì • ë¶„ì„ + ìŒì•… ë¸Œë¦¬í”„ ìƒì„±
- **ğŸµ ìŒì•… ìƒì„±**: í˜„ì¬ ì„¤ê³„ì•ˆ ê·¸ëŒ€ë¡œ ì‹¤ì œ ìŒì•… ìƒì„±
"""

def get_tips_md():
    return """### ğŸ’¡ íŒ
- ì„¤ê³„ì•ˆì´ ë§ˆìŒì— ë“¤ë©´ **ìŒì•… ìƒì„±**ì„ ëˆ„ë¥´ë©´ ê·¸ ì„¤ê³„ì•ˆìœ¼ë¡œ ìŒì•…ì´ ë§Œë“¤ì–´ì ¸ìš”.
- **ê°™ì€ ìŠ¤í† ë¦¬/ì´ë¯¸ì§€**ë©´ ì´ì „ ê°ì •ë¶„ì„ì„ ì¬ì‚¬ìš©í•´ìš”.
- ìŠ¤í† ë¦¬ë‚˜ ì´ë¯¸ì§€ë¥¼ ë°”ê¾¸ë©´ ì´ì „ ê²°ê³¼ëŠ” ì´ˆê¸°í™”ë¼ìš”.
"""


def on_text_tab_select():
    # ì§€ê¸ˆë¶€í„° í…ìŠ¤íŠ¸ ëª¨ë“œ
    app_state.current_mode = "text"
    # ë°˜ëŒ€í¸ ì…ë ¥(ì´ë¯¸ì§€)ë§Œ ë¹„ìš°ê³ , ê²°ê³¼ ì´ˆê¸°í™”
    app_state.image_path = None
    app_state.emotion_result = None
    app_state.music_brief = None
    return (
        gr.update(),             # story_input: ìœ ì§€ (ë°”ê¾¸ì§€ ì•ŠìŒ)
        gr.update(value=None),   # image_input: ë¹„ìš°ê¸°
        "",                      # emotion_output
        "",                      # brief_output
        None,                    # audio_output
        "ğŸ“ í…ìŠ¤íŠ¸ íƒ­ìœ¼ë¡œ ì „í™˜: ì´ì „ ì´ë¯¸ì§€ ì…ë ¥ê³¼ ê²°ê³¼ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤."
    )

def on_image_tab_select():
    # ì§€ê¸ˆë¶€í„° ì´ë¯¸ì§€ ëª¨ë“œ
    app_state.current_mode = "image"
    # ë°˜ëŒ€í¸ ì…ë ¥(í…ìŠ¤íŠ¸)ë§Œ ë¹„ìš°ê³ , ê²°ê³¼ ì´ˆê¸°í™”
    app_state.user_story = None
    app_state.emotion_result = None
    app_state.music_brief = None
    return (
        gr.update(value=""),     # story_input: ë¹„ìš°ê¸°
        gr.update(),             # image_input: ìœ ì§€ (ë°”ê¾¸ì§€ ì•ŠìŒ)
        "",                      # emotion_output
        "",                      # brief_output
        None,                    # audio_output
        "ğŸ–¼ï¸ ì´ë¯¸ì§€ íƒ­ìœ¼ë¡œ ì „í™˜: ì´ì „ í…ìŠ¤íŠ¸ ì…ë ¥ê³¼ ê²°ê³¼ë¥¼ ì´ˆê¸°í™”í–ˆìŠµë‹ˆë‹¤."
    )



class AppState:
    def __init__(self):
        self.emotion_result = None
        self.music_brief = None
        self.user_story = None
        self.image_path = None
        self.current_mode = "text"  # "text" or "image"

    def clear(self):
        self.emotion_result = None
        self.music_brief = None
        self.user_story = None
        self.image_path = None
        self.current_mode = "text"

    def set_story(self, s: str):
        self.user_story = _norm(s)
        self.current_mode = "text"

    def set_image(self, img_path: str):
        self.image_path = img_path
        self.current_mode = "image"

    def has_emotion_analysis(self, current_story=None, current_image=None):
        if self.current_mode == "text":
            return (self.emotion_result is not None and 
                   self.user_story == _norm(current_story or ""))
        else:  # image mode
            return (self.emotion_result is not None and 
                   self.image_path == current_image)

    def has_music_brief(self, current_story=None, current_image=None):
        if self.current_mode == "text":
            return (self.music_brief is not None and 
                   self.user_story == _norm(current_story or ""))
        else:  # image mode
            return (self.music_brief is not None and 
                   self.image_path == current_image)

# ë°˜ë³µë˜ëŠ” ì½”ë“œ í•¨ìˆ˜ë¡œ ì •ì˜
def _join(items):
    return ", ".join(items) if items else "â€”"

def md_emotion(emo: EmotionResult) -> str:
    return f"""**ğŸ­ ì£¼ìš” ê°ì •**: {emo.primary}

**ğŸ“Š ê°ì • ê°•ë„ (Valence)**: {emo.valence:.2f}
*(-1: ë§¤ìš° ë¶€ì •ì  â†” +1: ë§¤ìš° ê¸ì •ì )*

**âš¡ ê°ì„±ë„ (Arousal)**: {emo.arousal:.2f}
*(0: ì°¨ë¶„í•¨ â†” 1: í¥ë¶„ë¨)*

**ğŸ¯ ì‹ ë¢°ë„**: {emo.confidence:.2f}

**ğŸ’­ ë¶„ì„ ê·¼ê±°**:
{emo.reasons}"""

def md_brief(brief: MusicBrief) -> str:
    return f"""**ğŸµ ìŒì•… ë¶„ìœ„ê¸°**: {brief.mood}

**ğŸ¥ BPM**: {brief.bpm}

**ğŸ¼ ì¡°ì„±**: {brief.key}

**â±ï¸ ê¸¸ì´**: {brief.duration_sec}ì´ˆ

**ğŸ¹ ì•…ê¸°**: {_join(brief.instruments)}

**ğŸ·ï¸ ìŠ¤íƒ€ì¼ íƒœê·¸**: {_join(brief.style_tags)}

**ğŸ“ ìƒì„± í”„ë¡¬í”„íŠ¸**:
{brief.prompt}"""



# Global app state instance
app_state = AppState()

def create_gradio_interface():
    """Create and configure the Gradio interface"""
    
    def analyze_emotion_only(user_story, image_input):
        """
        Only analyze emotion without generating music
        """
        try:
            if image_input is not None:
                # Image mode
                if app_state.image_path != image_input:
                    app_state.clear()
                    app_state.set_image(image_input)
                
                if app_state.has_emotion_analysis(current_image=image_input):
                    emo = app_state.emotion_result
                    emotion_text = md_emotion(emo)
                    return emotion_text, "", None, "â™»ï¸ ì´ì „ ê°ì • ë¶„ì„ ê²°ê³¼ ì¬ì‚¬ìš© (ì´ë¯¸ì§€)"
                
                # Run image emotion analysis
                state = {"image_path": image_input}
                state = analyze_emotion_from_image_node(state)
                app_state.emotion_result = state.get("emotion")
                
                emotion_text = md_emotion(app_state.emotion_result)
                status = "âœ… ì´ë¯¸ì§€ ê°ì • ë¶„ì„ ì™„ë£Œ!"
                
            else:
                # Text mode
                if not _norm(user_story):
                    return "ì…ë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.", "", None, "âš ï¸ ì…ë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."

                if app_state.user_story != _norm(user_story):
                    app_state.clear()
                    app_state.set_story(user_story)

                if app_state.has_emotion_analysis(current_story=user_story):
                    emo = app_state.emotion_result
                    emotion_text = md_emotion(emo)
                    return emotion_text, "", None, "â™»ï¸ ì´ì „ ê°ì • ë¶„ì„ ê²°ê³¼ ì¬ì‚¬ìš© (í…ìŠ¤íŠ¸)"
                
                # Run text emotion analysis
                state = {"user_text": user_story, "force_generate": False}
                state = analyze_emotion_node(state)
                app_state.emotion_result = state.get("emotion")
                
                emotion_text = md_emotion(app_state.emotion_result)
                status = "âœ… í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ ì™„ë£Œ!"
            
            return emotion_text, "", None, status
            
        except Exception as e:
            error_msg = f"âŒ ê°ì • ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            return "ì˜¤ë¥˜ ë°œìƒ", "", None, error_msg
    
    def generate_music_brief_only(user_story, image_input):
        """
        Analyze emotion and generate music brief without actual music generation
        """
        try:
            if image_input is not None:
                # Image mode
                if app_state.image_path != image_input:
                    app_state.clear()
                    app_state.set_image(image_input)
                
                state = {"image_path": image_input}
                
                if app_state.has_emotion_analysis(current_image=image_input):
                    state["emotion"] = app_state.emotion_result
                else:
                    state = analyze_emotion_from_image_node(state)
                    app_state.emotion_result = state.get("emotion")
                
                state = image_compose_brief_node(state)
                app_state.music_brief = state.get("brief")
                
                status = "âœ… ì´ë¯¸ì§€ ê°ì • ë¶„ì„ ë° ìŒì•… ë¸Œë¦¬í”„ ìƒì„± ì™„ë£Œ!"
                
            else:
                # Text mode
                if not _norm(user_story):
                    return "ì…ë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.", "", None, "âš ï¸ ì…ë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."
                
                if app_state.user_story != _norm(user_story):
                    app_state.clear()
                    app_state.set_story(user_story)

                state = {"user_text": user_story, "force_generate": False}
                
                if app_state.has_emotion_analysis(current_story=user_story):
                    state["emotion"] = app_state.emotion_result
                else:
                    state = analyze_emotion_node(state)
                    app_state.emotion_result = state.get("emotion")
                
                state = compose_brief_node(state)
                app_state.music_brief = state.get("brief")
                
                status = "âœ… í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„ ë° ìŒì•… ë¸Œë¦¬í”„ ìƒì„± ì™„ë£Œ!"
            
            emotion_text = md_emotion(app_state.emotion_result)
            brief_text = md_brief(app_state.music_brief)
            
            return emotion_text, brief_text, None, status
            
        except Exception as e:
            error_msg = f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            return "ì˜¤ë¥˜ ë°œìƒ", "ì˜¤ë¥˜ ë°œìƒ", None, error_msg
    
    def generate_full_music(user_story, image_input):
        """
        Generate full music from text or image input
        """
        try:
            # íƒ­ ì—…ë°ì´íŠ¸ ê¸°ë³¸ê°’(ë³€ê²½ ì—†ìŒ)
            tabs_update = gr.update()

            # â˜… guard: í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ë‘˜ ë‹¤ ì—†ëŠ” ê²½ìš°
            if (image_input is None) and (not _norm(user_story)):
                return ("ì…ë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.",
                        "", None, "âš ï¸ ì…ë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.", tabs_update)

            if image_input is not None:
                # Image mode
                if app_state.image_path != image_input:
                    app_state.clear()
                    app_state.set_image(image_input)
            else:
                # Text mode
                if not _norm(user_story):
                    return ("ì…ë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”.", "", None, "âš ï¸ ì…ë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.", tabs_update)
                if app_state.user_story != _norm(user_story):
                    app_state.clear()
                    app_state.set_story(user_story)

            # í™˜ê²½ ì²´í¬
            openai_ok = bool(os.getenv("OPENAI_API_KEY"))
            repl_ok   = bool(os.getenv("REPLICATE_API_TOKEN"))
            use_repl  = os.getenv("USE_REPLICATE", "0") == "1"
            if not openai_ok:
                return "OpenAI í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.", "", None, "âŒ OPENAI_API_KEY í•„ìš”", tabs_update
            if not (repl_ok and use_repl):
                # ìŒì•… í•©ì„± ê±´ë„ˆë›°ê³  ë¶„ì„/ë¸Œë¦¬í”„ë§Œ
                if app_state.current_mode == "image":
                    if not app_state.has_emotion_analysis(current_image=image_input):
                        st = {"image_path": image_input}
                        st = analyze_emotion_from_image_node(st)
                        app_state.emotion_result = st["emotion"]
                    if not app_state.has_music_brief(current_image=image_input):
                        st = {"image_path": image_input, "emotion": app_state.emotion_result}
                        st = image_compose_brief_node(st)
                        app_state.music_brief = st["brief"]
                else:
                    if not app_state.has_emotion_analysis(current_story=user_story):
                        st = {"user_text": user_story}
                        st = analyze_emotion_node(st)
                        app_state.emotion_result = st["emotion"]
                    if not app_state.has_music_brief(current_story=user_story):
                        st = {"user_text": user_story, "emotion": app_state.emotion_result}
                        st = compose_brief_node(st)
                        app_state.music_brief = st["brief"]

                emo, brief = app_state.emotion_result, app_state.music_brief
                emotion_text = md_emotion(emo)
                brief_text   = md_brief(brief)
                return emotion_text, brief_text, None, "âš ï¸ USE_REPLICATE=1 ë˜ëŠ” REPLICATE í† í° ì—†ìŒ â†’ ìŒì•… ìƒì„± ê±´ë„ˆëœ€", tabs_update

            # â”€â”€ ë¶„ê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if (app_state.has_emotion_analysis(current_story=user_story, current_image=image_input) and 
                app_state.has_music_brief(current_story=user_story, current_image=image_input)):
                # â–¶ ë¶„ì„/ë¸Œë¦¬í”„ ì¬ì‚¬ìš© â†’ í•©ì„±ë§Œ
                brief = app_state.music_brief
                audio_path = generate_with_replicate_strict(brief.prompt, int(brief.duration_sec))
                audio_path, _ = _rename_generated_file(audio_path, brief)
                audio_ui = gr.update(value=audio_path, label=_label_with_ext("ì¹˜ë£Œìš© ìŒì•…", audio_path))
                status = f"ğŸµ ê¸°ì¡´ ë¶„ì„/ë¸Œë¦¬í”„ ì¬ì‚¬ìš©í•˜ì—¬ ìŒì•… ìƒì„± ({app_state.current_mode})"
                tabs_update = gr.update(selected=0)

            elif (app_state.has_emotion_analysis(current_story=user_story, current_image=image_input) and 
                not app_state.has_music_brief(current_story=user_story, current_image=image_input)):
                # â–¶ ê°ì •ë§Œ ìˆìŒ â†’ ë¸Œë¦¬í”„ ìƒì„± â†’ í•©ì„±
                if app_state.current_mode == "image":
                    st = {"image_path": image_input, "emotion": app_state.emotion_result}
                    st = image_compose_brief_node(st)
                else:
                    st = {"user_text": user_story, "emotion": app_state.emotion_result}
                    st = compose_brief_node(st)
                app_state.music_brief = st["brief"]
                brief = app_state.music_brief
                audio_path = generate_with_replicate_strict(brief.prompt, int(brief.duration_sec))
                audio_path, _ = _rename_generated_file(audio_path, brief)
                audio_ui = gr.update(value=audio_path, label=_label_with_ext("ì¹˜ë£Œìš© ìŒì•…", audio_path))
                status = f"ğŸ§© ê¸°ì¡´ ê°ì • ë¶„ì„ ì¬ì‚¬ìš© â†’ ë¸Œë¦¬í”„ ìƒì„± â†’ ìŒì•… ìƒì„± ({app_state.current_mode})"
                tabs_update = gr.update(selected=0)

            else:
                # â–¶ ì›ìƒ·
                if app_state.current_mode == "image":
                    st = {"image_path": image_input}
                    final = image_graph.invoke(st)
                    emo = final["emotion"]; brief = final["brief"]
                    app_state.emotion_result = emo
                    app_state.music_brief = brief
                    audio_path = generate_with_replicate_strict(brief.prompt, int(brief.duration_sec))
                    audio_path, _ = _rename_generated_file(audio_path, brief)
                    audio_ui = gr.update(value=audio_path, label=_label_with_ext("ì¹˜ë£Œìš© ìŒì•…", audio_path))
                else:
                    st = {"user_text": user_story, "force_generate": True}
                    final = graph.invoke(st)
                    emo = final["emotion"]; brief = final["brief"]
                    app_state.emotion_result = emo
                    app_state.music_brief = brief
                    audio_path = final.get("audio_path")
                    audio_path, _ = _rename_generated_file(audio_path, brief)
                    audio_ui = gr.update(value=audio_path, label=_label_with_ext("ì¹˜ë£Œìš© ìŒì•…", audio_path))
                status = f"ğŸš€ ì›ìƒ· ìƒì„± ({app_state.current_mode} ëª¨ë“œ)"
                tabs_update = gr.update(selected=0)

            # ì¶œë ¥ êµ¬ì„±
            emo = app_state.emotion_result
            brief = app_state.music_brief
            emotion_text = md_emotion(emo)
            brief_text   = md_brief(brief)
            return emotion_text, brief_text, audio_ui, status, tabs_update

        except Exception as e:
            return "ì˜¤ë¥˜ ë°œìƒ", "ì˜¤ë¥˜ ë°œìƒ", None, f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}", gr.update()

    
    # Create the Gradio interface
    with gr.Blocks(
        title="ì¹˜ë£Œìš© ìŒì•… ìƒì„± AI",
        theme=gr.themes.Soft(),
        css="""
        .main-container { max-width: 1200px; margin: 0 auto; }
        .story-input { min-height: 150px; }
        .result-box { border: 1px solid #e0e0e0; border-radius: 8px; padding: 15px; margin: 10px 0; }
        .step-button { margin: 5px; }

        /* ì‚¬ìš©ë²•/íŒ ì¹´ë“œ */
        .info-card {
        border: 1px solid #e5e7eb;
        border-radius: 12px;
        background: #ffffff;
        padding: 14px 16px;
        transition: transform .12s ease, box-shadow .12s ease, border-color .12s ease;
        }

        /* ì œëª©/ë³¸ë¬¸ ê°€ë…ì„± ê°•í™” */
        .info-card h3 {
        margin: 0 0 8px 0;
        font-size: 16px;
        font-weight: 800;
        color: #111827;
        }
        .info-card p, .info-card li {
        color: #111827;
        line-height: 1.55;
        }

        /* â”€â”€â”€â”€â”€â”€â”€â”€â”€ Gradio ë²„íŠ¼ ìƒ‰ìƒ(ë³€ìˆ˜) ì˜¤ë²„ë¼ì´ë“œ: elem_id ë²”ìœ„ ë‚´ì—ì„œë§Œ ì ìš© â”€â”€â”€â”€â”€â”€â”€â”€â”€ */

        /* ê°ì • ë¶„ì„(ì´ˆë¡ í‹´íŠ¸) â€” secondary ë³€í˜• */
        #btn-emotion {
        --button-secondary-background-fill: #F0FDF4;          /* ê¸°ë³¸ ë°°ê²½ */
        --button-secondary-text-color:       #065F46;          /* í…ìŠ¤íŠ¸ */
        --button-secondary-border-color:     #4ADE80;          /* í…Œë‘ë¦¬ */
        --button-secondary-background-fill-hover: #DCFCE7;     /* í˜¸ë²„ ë°°ê²½ */
        --button-secondary-border-color-hover:     #22C55E;    /* í˜¸ë²„ í…Œë‘ë¦¬ */
        --button-shadow: 0 1px 0 rgba(0,0,0,.03);
        }

        /* ê°ì • ë¶„ì„ + ìŒì•… ì„¤ê³„(ì˜¤ë Œì§€ í‹´íŠ¸) â€” secondary ë³€í˜• */
        #btn-brief {
        --button-secondary-background-fill: #FFF7ED;
        --button-secondary-text-color:       #9A3412;
        --button-secondary-border-color:     #FB923C;
        --button-secondary-background-fill-hover: #FFEDD5;
        --button-secondary-border-color-hover:     #F97316;
        --button-shadow: 0 1px 0 rgba(0,0,0,.03);
        }

        /* ë©”ì¸ CTA: ìŒì•… ìƒì„±(íŒŒë‘) â€” primary ë³€í˜• */
        #btn-generate {
        --button-primary-background-fill: #2563EB;
        --button-primary-text-color:      #FFFFFF;
        --button-primary-border-color:    #1D4ED8;
        --button-primary-background-fill-hover: #1D4ED8;
        --button-shadow: 0 8px 18px rgba(37,99,235,0.28);
        }

        /* í¬ê¸°/ìœ¤ê³½/ì „í™˜ íš¨ê³¼(ë²„íŠ¼ì²˜ëŸ¼ ë³´ì´ê²Œ) â€” êµ¬ì¡° ë³€í™”ì—ë„ ì˜ ë¨¹íˆë„ë¡ ë„“ì€ ì„ íƒì */
        #btn-emotion button, #btn-emotion .gr-button, #btn-emotion [role="button"],
        #btn-brief   button, #btn-brief   .gr-button, #btn-brief   [role="button"],
        #btn-generate button, #btn-generate .gr-button, #btn-generate [role="button"] {
        height: 44px;
        min-width: 200px;
        padding: 0 16px;
        border-radius: 10px;
        font-weight: 700;
        letter-spacing: .01em;
        display: inline-flex;
        align-items: center;
        justify-content: center;
        transition: background .12s ease, border-color .12s ease,
                    box-shadow .12s ease, transform .06s ease;
        }

        /* í¬ì»¤ìŠ¤ ì ‘ê·¼ì„± ë§ */
        #btn-emotion button:focus-visible,
        #btn-brief button:focus-visible,
        #btn-generate button:focus-visible {
        outline: 3px solid rgba(59,130,246,.45);
        outline-offset: 2px;
        }

        /* í˜¸ë²„ ì‹œ ì‚´ì§ ìƒìŠ¹ íš¨ê³¼(CTAë§Œ ë” ê°•ì¡°) */
        #btn-generate button:hover,
        #btn-generate .gr-button:hover,
        #btn-generate [role="button"]:hover {
        transform: translateY(-1px);
        }
        #btn-generate button:active,
        #btn-generate .gr-button:active,
        #btn-generate [role="button"]:active {
        transform: translateY(0);
        }

        /* ëª¨ë°”ì¼: ê½‰ ì°¨ê²Œ */
        @media (max-width: 768px) {
        #btn-emotion button, #btn-emotion .gr-button, #btn-emotion [role="button"],
        #btn-brief   button, #btn-brief   .gr-button, #btn-brief   [role="button"],
        #btn-generate button, #btn-generate .gr-button, #btn-generate [role="button"] {
            width: 100%;
            min-width: 0;
        }
        }

        """
    ) as demo:
        
        gr.Markdown("""
        # ğŸµ ê°œì¸ ê°ì • ìŠ¤í† ë¦¬ ê¸°ë°˜ ì¹˜ë£Œìš© ìŒì•… ìƒì„± AI
        
        **ë‹¹ì‹ ì˜ ì´ì•¼ê¸°ì™€ ê°ì •ì„ ë¶„ì„í•˜ì—¬ ë§ì¶¤í˜• ì¹˜ë£Œ ìŒì•…ì„ ìƒì„±í•©ë‹ˆë‹¤**
        
        ì´ AIëŠ” ì˜ˆìˆ ì¹˜ë£Œ ë° ì‹¬ë¦¬ì•ˆì • ì§€ì›ì„ ëª©ì ìœ¼ë¡œ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. 
        ë‹¹ì‹ ì˜ ê°ì • ìƒíƒœë¥¼ ë¶„ì„í•˜ê³ , ê·¸ì— ë§ëŠ” ì¹˜ë£Œì  ìŒì•…ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """)
        
        with gr.Row():
            with gr.Column(scale=2):
                # ğŸ‘‡ íƒ­ì„ ì „ì²´ ë„ˆë¹„ë¡œ ì˜¬ë ¤ì„œ ë°‘ì¤„ì´ ì¢Œìš° ëê¹Œì§€ ë³´ì´ê²Œ
                gr.Markdown("## ğŸ”½ ì…ë ¥ ë°©ì‹ì„ ì„ íƒí•˜ì„¸ìš”")

                with gr.Tabs() as input_tabs:
                    with gr.Tab("ğŸ“ í…ìŠ¤íŠ¸") as text_tab:   # â† ë³€ìˆ˜ë¡œ ë°›ê¸°
                        with gr.Row():
                            with gr.Column(scale=2):
                                story_input = gr.Textbox(
                                    label="ê°ì •ì´ë‚˜ ìƒí™©ì„ ììœ ë¡­ê²Œ ì¨ì£¼ì„¸ìš”",
                                    placeholder="ì˜ˆ: ì˜¤ëŠ˜ í•˜ë£¨ ì¢…ì¼ ë§ˆìŒì´ ë¬´ê±°ì› ë‹¤...",
                                    lines=6,
                                    elem_classes=["story-input"]
                                )
                            with gr.Column(scale=1):
                                gr.Markdown(get_usage_md(), elem_classes=["info-card"])
                                gr.Markdown(get_tips_md(),  elem_classes=["info-card"])

                    with gr.Tab("ğŸ–¼ï¸ ì´ë¯¸ì§€") as image_tab: # â† ë³€ìˆ˜ë¡œ ë°›ê¸°
                        with gr.Row():
                            with gr.Column(scale=2):
                                image_input = gr.Image(
                                    label="ê°ì •ì„ í‘œí˜„í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”",
                                    type="filepath",
                                    sources=["upload", "clipboard"],
                                    height=300
                                )
                            with gr.Column(scale=1):
                                gr.Markdown(get_usage_md(), elem_classes=["info-card"])
                                gr.Markdown(get_tips_md(),  elem_classes=["info-card"])

                # ğŸ‘‡ ë²„íŠ¼ë“¤ì€ íƒ­(ë°‘ì¤„) ì•„ë˜ì— ê·¸ëŒ€ë¡œ ë†“ì„ (ì „ì²´ ë„ˆë¹„)
                gr.Markdown("## ğŸ¯ ì›í•˜ëŠ” ì‘ì—…ì„ ì„ íƒí•˜ì„¸ìš”")
                with gr.Row():
                    emotion_only_btn = gr.Button(
                        "ğŸ” ê°ì • ë¶„ì„",
                        variant="secondary",           # â† í•˜ì–€/ì•„ì›ƒë¼ì¸
                        scale=1,
                        elem_classes=["step-button"],
                        elem_id="btn-emotion"          # â† CSS íƒ€ê²ŸíŒ…ìš©
                    )
                    brief_only_btn   = gr.Button(
                        "ğŸ“‹ ê°ì • ë¶„ì„ + ìŒì•… ì„¤ê³„",
                        variant="secondary",           # â† í•˜ì–€/ì•„ì›ƒë¼ì¸ (ë³€ê²½ í¬ì¸íŠ¸)
                        scale=1,
                        elem_classes=["step-button"],
                        elem_id="btn-brief"            # â† CSS íƒ€ê²ŸíŒ…ìš©
                    )
                    full_generate_btn= gr.Button(
                        "ğŸµ ìŒì•… ìƒì„±",
                        variant="primary",             # â† íŒŒë€ìƒ‰ ìœ ì§€
                        scale=1,
                        elem_classes=["step-button"],
                        elem_id="btn-generate"         # â† CSS íƒ€ê²ŸíŒ…ìš©
                    )

        
        # Results section
        gr.Markdown("## ğŸ“Š ë¶„ì„ ê²°ê³¼")
        
        with gr.Row():
            with gr.Column():
                emotion_output = gr.Markdown(
                    label="ê°ì • ë¶„ì„ ê²°ê³¼",
                    elem_classes=["result-box"]
                )
            
            with gr.Column():
                brief_output = gr.Markdown(
                    label="ìŒì•… ë¸Œë¦¬í”„",
                    elem_classes=["result-box"]
                )
        
        # Audio output and download
        gr.Markdown("## ğŸµ ìƒì„±/ì¬ìƒ")

        with gr.Tabs() as audio_tabs:
            with gr.Tab("ğŸµ ìƒì„±ëœ ìŒì•…"):
                audio_output = gr.Audio(
                    label="ì¹˜ë£Œìš© ìŒì•…",
                    type="filepath",
                    autoplay=False,
                    loop=True,              # ëë‚˜ë©´ ë‹¤ì‹œ ì²˜ìŒë¶€í„°
                    interactive=False,      # ì¬ìƒë§Œ, í¸ì§‘ ë¶ˆê°€
                    editable=False,         # ìë¥´ê¸°/ë˜ëŒë¦¬ê¸° ë„êµ¬ ìˆ¨ê¹€
                    show_download_button=True,
                    elem_id="music_player"   # â† ì»¤ìŠ¤í…€ ì»¨íŠ¸ë¡¤ì´ ì°¾ì„ ID
                )

            with gr.Tab("ğŸ“ ë‚´ íŒŒì¼ ì¬ìƒ"):
                user_audio = gr.Audio(
                    label="ë‚´ íŒŒì¼ ì—…ë¡œë“œ(ë“œë˜ê·¸&ë“œë¡­)",
                    sources=["upload"],      # íŒŒì¼ ì„ íƒ + ë“œë˜ê·¸&ë“œë¡­
                    type="filepath",         # í¸ì§‘ í›„ì—ë„ íŒŒì¼ ê²½ë¡œë¡œ ë°˜í™˜
                    autoplay=False,
                    loop=True,               # ì—…ë¡œë“œ íŒŒì¼ë„ ë°˜ë³µ ì¬ìƒ
                    interactive=True,        # ì‚¬ìš©ì ì¡°ì‘ í—ˆìš©
                    editable=True,           # âœ‚ï¸ ìë¥´ê¸° / â†© ë˜ëŒë¦¬ê¸° / â†ª ë‹¤ì‹œí•˜ê¸° ë³´ì´ê¸°
                    show_download_button=True,
                    elem_id="user_player"    # ì»¤ìŠ¤í…€ ì»¨íŠ¸ë¡¤ìš© ID
                )
        
        status_output = gr.Markdown("")

        def _on_user_audio_change(path):
            if not path:
                return gr.update()
            name = os.path.basename(path)
            return gr.update(label=f"ë‚´ íŒŒì¼ ì—…ë¡œë“œ â€” {name}")
        
        user_audio.change(
            fn=_on_user_audio_change,
            inputs=[user_audio],
            outputs=[user_audio]
        )

        
        emotion_only_btn.click(
            fn=analyze_emotion_only,
            inputs=[story_input, image_input],
            outputs=[emotion_output, brief_output, audio_output, status_output]
        )
        
        brief_only_btn.click(
            fn=generate_music_brief_only,
            inputs=[story_input, image_input],
            outputs=[emotion_output, brief_output, audio_output, status_output]
        )
        
        full_generate_btn.click(
            fn=generate_full_music,
            inputs=[story_input, image_input],
            outputs=[emotion_output, brief_output, audio_output, status_output, audio_tabs],
        )

        text_tab.select(
            fn=on_text_tab_select,
            inputs=[],
            outputs=[story_input, image_input, emotion_output, brief_output, audio_output, status_output]
        )

        image_tab.select(
            fn=on_image_tab_select,
            inputs=[],
            outputs=[story_input, image_input, emotion_output, brief_output, audio_output, status_output]
        )


        
        # Footer
        gr.Markdown("""
        ---
        **ê°œë°œ ì •ë³´**: ì´ AIëŠ” ê°œì¸ì˜ ê°ì •ê³¼ ìŠ¤í† ë¦¬ë¥¼ ë‹´ì€ í¼ìŠ¤ë„ ë®¤ì§ ìƒì„±ì„ í†µí•´ ì˜ˆìˆ ì¹˜ë£Œ ë° ì‹¬ë¦¬ì•ˆì •ì„ ì§€ì›í•©ë‹ˆë‹¤.
        
        **ì£¼ì˜ì‚¬í•­**: 
        - ì´ ë„êµ¬ëŠ” ì „ë¬¸ì ì¸ ì‹¬ë¦¬ì¹˜ë£Œë¥¼ ëŒ€ì²´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤
        - ì‹¬ê°í•œ ì •ì‹ ê±´ê°• ë¬¸ì œê°€ ìˆë‹¤ë©´ ì „ë¬¸ê°€ì˜ ë„ì›€ì„ ë°›ìœ¼ì‹œê¸° ë°”ëë‹ˆë‹¤
        - ìƒì„±ëœ ìŒì•…ì€ ê°œì¸ì  ìš©ë„ë¡œë§Œ ì‚¬ìš©í•´ì£¼ì„¸ìš”
        """)
    
    return demo

if __name__ == "__main__":
    # Create and launch the interface
    demo = create_gradio_interface()
    
    # Launch with sharing enabled for easy access
    demo.launch(
        server_name="0.0.0.0",  # Allow external access
        server_port=7860,       # Default Gradio port
        share=False,             # Create public link
        debug=True              # Enable debug mode
    )
